<style>
/* Reuse styles or add specific styles */
.formula {
  background-color: #f8f8f8;
  border: 1px solid #ddd;
  border-radius: 4px;
  padding: 1em;
  margin: 1em 0;
  overflow-x: auto;
  font-family: monospace;
}
.algorithm-box {
  border: 1px solid #eee;
  border-radius: 8px;
  padding: 1rem;
  margin-bottom: 1rem;
  background-color: #fafafa;
}
.algorithm-box h4 {
  margin-top: 0;
  color: #333;
}
</style>

<h1 align="center" id="机器学习基础">机器学习基础 (不含强化学习)</h1>

**概要：** 本章节将深入探讨机器学习（ML）的基础概念，重点介绍监督学习和无监督学习这两大核心范式，涵盖常用算法、模型评估方法以及关键挑战。强化学习将在 [后续章节](/AI/03_强化学习(RL)) 详细讨论。

## 1. 回顾：什么是机器学习？

如 [AI 基础与历史](/AI/01_基础与历史) 中所述，机器学习是 AI 的一个核心子领域，专注于开发能够让计算机系统从**数据**中**学习**并改进性能的算法，而无需进行显式编程。其目标是让机器能够自动发现数据中的模式，并利用这些模式进行预测、分类或决策。

本章将重点关注除强化学习之外的两种主要 ML 范式：监督学习和无监督学习。

## 2. 监督学习 (Supervised Learning)

监督学习是最常见和研究最深入的机器学习范式。其核心思想是**从带有标签的训练数据中学习一个映射函数**，该函数可以将输入数据映射到正确的输出标签。

-   **训练数据:** 包含成对的输入特征 (X) 和对应的期望输出标签 (y)。标签是由“监督者”（通常是人类专家）提供的。
-   **目标:** 学习一个模型 `f`，使得对于新的、未见过的输入 `x_new`，模型预测的输出 `f(x_new)` 能够尽可能接近真实的标签 `y_new`。

监督学习主要解决两类问题：**分类 (Classification)** 和 **回归 (Regression)**。

### 2.1 分类 (Classification)

-   **目标:** 将输入数据划分到预定义的**离散类别 (Categories)** 中的某一个。
-   **输出标签 (y):** 离散值，代表不同的类别（例如，“猫”/“狗”，"垃圾邮件”/“非垃圾邮件”，"数字 0-9"）。
-   **常用算法:**
    *   **逻辑回归 (Logistic Regression):** 虽然名字带“回归”，但主要用于二分类问题。它通过 Sigmoid 函数将线性回归的输出映射到 (0, 1) 区间，表示属于某个类别的概率。
    *   **支持向量机 (Support Vector Machine, SVM):** 寻找一个最优的超平面（在高维空间中）将不同类别的数据点分开，目标是最大化不同类别数据点到该超平面的间隔 (Margin)。适用于线性和非线性分类（通过核技巧）。
    *   **决策树 (Decision Tree):** 构建一个树状结构，每个内部节点表示一个特征测试，每个分支代表测试结果，每个叶节点代表一个类别标签。易于理解和解释。
    *   **随机森林 (Random Forest):** 集成学习方法，通过构建多个决策树并综合它们的预测结果（如投票）来提高性能和鲁棒性。
    *   **K-近邻 (K-Nearest Neighbors, KNN):** 基于实例的学习，预测新数据点的类别时，查看其在特征空间中最近的 K 个邻居，根据邻居中最常见的类别进行预测。
    *   **朴素贝叶斯 (Naive Bayes):** 基于贝叶斯定理和特征条件独立假设的概率分类器。计算简单，在文本分类等领域效果良好。
-   **例子:** 图像识别（判断图片是猫还是狗）、垃圾邮件检测、文本情感分析（判断评论是正面还是负面）、疾病诊断。

### 2.2 回归 (Regression)

-   **目标:** 预测一个**连续值 (Continuous Value)** 的输出。
-   **输出标签 (y):** 连续值（例如，房价、温度、股票价格）。
-   **常用算法:**
    *   **线性回归 (Linear Regression):** 最简单的回归模型，试图找到一条直线（或超平面）来最佳拟合输入特征和输出值之间的关系。假设输出是输入特征的线性组合。
        <div class="formula">y = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b</div>
        其中 `w` 是权重，`b` 是偏置。
    *   **多项式回归 (Polynomial Regression):** 线性回归的扩展，允许输入特征的更高次项，可以拟合非线性关系。
    *   **岭回归 (Ridge Regression) / Lasso 回归:** 线性回归的正则化版本，通过在损失函数中添加惩罚项来防止过拟合，有助于处理特征之间存在多重共线性的情况。Lasso 还能进行特征选择（使部分特征权重变为 0）。
    *   **支持向量回归 (Support Vector Regression, SVR):** SVM 的回归版本，目标是找到一个函数，使得尽可能多的数据点落在围绕该函数的某个“管道”内。
    *   **决策树 / 随机森林:** 也可用于回归任务，叶节点预测的是一个连续值（通常是该叶节点所有训练样本的平均值）。
-   **例子:** 预测房价、预测股票价格、预测气温、根据广告投入预测销售额。

## 3. 无监督学习 (Unsupervised Learning)

与监督学习不同，无监督学习处理的是**没有标签**的训练数据。其目标是**发现数据中隐藏的结构、模式或关系**。

-   **训练数据:** 只包含输入特征 (X)，没有对应的输出标签 (y)。
-   **目标:** 理解数据的内在结构，例如将相似的数据点分组（聚类）或降低数据的维度（降维）。

无监督学习主要解决两类问题：**聚类 (Clustering)** 和 **降维 (Dimensionality Reduction)**。

### 3.1 聚类 (Clustering)

-   **目标:** 将数据集划分为若干个组（簇），使得同一簇内的数据点彼此相似，而不同簇的数据点彼此不相似。
-   **核心思想:** 根据数据点之间的相似性度量（如距离）进行分组。
-   **常用算法:**
    *   **K-均值 (K-Means):** 最常用的聚类算法之一。需要预先指定簇的数量 K。算法迭代地将每个数据点分配给最近的簇中心，并更新簇中心为其成员点的均值，直到簇中心不再变化或达到最大迭代次数。
    *   **层次聚类 (Hierarchical Clustering):** 构建数据点的嵌套聚类层次结构（树状图 Dendrogram）。可以是凝聚的（从单个点开始合并）或分裂的（从整个数据集开始划分）。
    *   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** 基于密度的聚类算法，能够发现任意形状的簇，并将噪声点识别出来。不需要预先指定簇的数量。
-   **例子:** 用户细分（将具有相似购买行为的用户分组）、图像分割（将图像中颜色或纹理相似的区域分组）、基因表达数据分析、异常检测（远离任何簇的点可能是异常点）。

### 3.2 降维 (Dimensionality Reduction)

-   **目标:** 在保留数据主要信息的前提下，减少数据的特征数量（维度）。
-   **动机:**
    *   **可视化:** 将高维数据降到 2D 或 3D 以便观察。
    *   **去除冗余和噪声:** 减少不相关或噪声特征的影响。
    *   **压缩数据:** 减少存储空间和计算时间。
    *   **克服“维度灾难”:** 在高维空间中，数据变得稀疏，许多机器学习算法的性能会下降。
-   **常用算法:**
    *   **主成分分析 (Principal Component Analysis, PCA):** 最常用的线性降维方法。通过线性变换将数据投影到方差最大的几个方向（主成分）上。
    *   **线性判别分析 (Linear Discriminant Analysis, LDA):** 一种**监督**降维方法（虽然常与无监督降维并列讨论），目标是找到一个投影方向，使得不同类别的数据点在投影后尽可能分开，同类数据点尽可能聚集。主要用于分类任务的预处理。
    *   **t-分布随机邻域嵌入 (t-SNE):** 非线性降维方法，特别擅长将高维数据映射到低维空间进行可视化，能够很好地保留数据的局部结构（相似的点在低维空间中也靠近）。
    *   **自编码器 (Autoencoders):** 基于神经网络的无监督降维方法。通过训练一个神经网络来重构其输入，其中间层的维度小于输入层，该中间层的输出即为降维后的表示。
-   **例子:** 人脸识别（从高维像素空间提取关键面部特征）、数据可视化、文本数据分析（降低词袋模型的维度）、生物信息学。

## 4. 模型评估 (Model Evaluation)

选择合适的算法并训练模型后，需要评估其性能。评估指标取决于任务类型（分类或回归）。

### 4.1 分类评估指标

通常基于**混淆矩阵 (Confusion Matrix)** 计算：

|                | 预测为正类 (Positive) | 预测为负类 (Negative) |
| -------------- | --------------------- | --------------------- |
| **实际为正类** | 真阳性 (TP)           | 假阴性 (FN)           |
| **实际为负类** | 假阳性 (FP)           | 真阴性 (TN)           |

-   **准确率 (Accuracy):** (TP + TN) / (TP + TN + FP + FN)
    *   含义：预测正确的样本占总样本的比例。
    *   缺点：在类别不平衡的数据集上具有误导性。
-   **精确率 (Precision):** TP / (TP + FP)
    *   含义：预测为正类的样本中，实际为正类的比例（查准率）。关注预测的准确性。
-   **召回率 (Recall) / 灵敏度 (Sensitivity):** TP / (TP + FN)
    *   含义：实际为正类的样本中，被正确预测为正类的比例（查全率）。关注是否漏掉了正类。
-   **F1 分数 (F1-Score):** 2 * (Precision * Recall) / (Precision + Recall)
    *   含义：精确率和召回率的调和平均数，综合考虑两者。
-   **AUC (Area Under the ROC Curve):** ROC 曲线（Receiver Operating Characteristic Curve）下的面积。ROC 曲线以假阳性率 (FPR = FP / (FP + TN)) 为横轴，真阳性率 (TPR = Recall) 为纵轴绘制。
    *   含义：衡量模型区分正负类能力的综合指标。AUC 越接近 1，模型性能越好；AUC = 0.5 表示随机猜测。

### 4.2 回归评估指标

-   **平均绝对误差 (Mean Absolute Error, MAE):** `(1/n) * Σ|y_true - y_pred|`
    *   含义：预测值与真实值之差的绝对值的平均数。易于理解，对异常值不敏感。
-   **均方误差 (Mean Squared Error, MSE):** `(1/n) * Σ(y_true - y_pred)^2`
    *   含义：预测值与真实值之差的平方的平均数。对误差进行平方，放大了较大误差的影响。
-   **均方根误差 (Root Mean Squared Error, RMSE):** `sqrt(MSE)`
    *   含义：MSE 的平方根，量纲与原始数据相同，更易于解释。
-   **R 平方 (R-squared) / 决定系数:** `1 - (Σ(y_true - y_pred)^2) / (Σ(y_true - y_mean)^2)`
    *   含义：模型解释的方差占总方差的比例。值越接近 1，表示模型拟合得越好；可能为负数（表示模型还不如直接预测平均值）。

**交叉验证 (Cross-Validation):** 为了更可靠地评估模型性能并避免在特定测试集上过拟合，通常使用交叉验证。例如，K 折交叉验证将数据分成 K 份，轮流使用其中 K-1 份训练，1 份测试，最后取 K 次评估结果的平均值。

## 5. 关键挑战：过拟合与欠拟合

在训练机器学习模型时，经常会遇到两个主要问题：

-   **欠拟合 (Underfitting):** 模型过于简单，未能捕捉到数据中的基本模式。表现为在训练集和测试集上性能都很差。
    *   **原因:** 模型复杂度不够、特征不足、训练不充分。
    *   **解决方法:** 增加模型复杂度（如使用更复杂的模型、增加网络层数/节点数）、添加更多特征、延长训练时间。
-   **过拟合 (Overfitting):** 模型过于复杂，不仅学习了数据中的普遍规律，还学习了训练数据特有的噪声或随机波动。表现为在训练集上性能极好，但在未见过的测试集上性能很差（泛化能力差）。
    *   **原因:** 模型复杂度过高、训练数据量不足、特征过多。
    *   **解决方法:**
        *   **增加数据量:** 获取更多训练数据。
        *   **数据增强 (Data Augmentation):** 对现有数据进行变换以生成更多样本。
        *   **降低模型复杂度:** 使用更简单的模型、减少网络层数/节点数。
        *   **正则化 (Regularization):** 在损失函数中添加惩罚项（如 L1, L2 正则化），限制模型参数的大小。
        *   **Dropout (用于神经网络):** 训练时随机丢弃一部分神经元，强制网络学习更鲁棒的特征。
        *   **早停法 (Early Stopping):** 在验证集上监控模型性能，当性能不再提升或开始下降时停止训练。
        *   **特征选择/降维:** 减少特征数量。

**偏差-方差权衡 (Bias-Variance Tradeoff):**
这两个问题与偏差和方差有关：
-   **偏差 (Bias):** 模型预测值与真实值之间的系统性差异（衡量模型的拟合能力）。高偏差通常导致欠拟合。
-   **方差 (Variance):** 模型在不同训练数据集上预测结果的变化程度（衡量模型对数据扰动的敏感度）。高方差通常导致过拟合。
机器学习的目标是在偏差和方差之间找到一个平衡点，使得总误差（通常认为是偏差^2 + 方差 + 不可避免的噪声误差）最小。

## 6. 总结

监督学习和无监督学习是机器学习的两大基石。监督学习通过带标签数据学习预测或分类，而无监督学习则从未标记数据中发现结构。理解这些范式下的核心概念、常用算法（如线性/逻辑回归、SVM、K-Means、PCA）、模型评估指标以及如何应对过拟合与欠拟合的挑战，是深入学习更高级 AI 技术（如深度学习、强化学习）的基础。

## 7. 参考资料

-   [机器学习 (周志华)](https://book.douban.com/subject/26708119/) - 国内经典机器学习教材
-   [Pattern Recognition and Machine Learning (Christopher Bishop)](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) - PRML，机器学习领域的经典之作
-   [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html) - 流行的 Python 机器学习库，包含大量算法实现和文档
