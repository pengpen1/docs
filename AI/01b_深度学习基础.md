<style>
/* Reuse styles or add specific styles */
.nn-diagram svg {
  max-width: 100%;
  height: auto;
}
.formula {
  background-color: #f8f8f8;
  border: 1px solid #ddd;
  border-radius: 4px;
  padding: 1em;
  margin: 1em 0;
  overflow-x: auto;
  font-family: monospace;
}
</style>

<h1 align="center" id="深度学习基础">深度学习基础</h1>

**概要：** 本章节将深入探讨深度学习（Deep Learning, DL）的核心概念和机制，包括人工神经网络的基本结构、信息传播方式（前向与反向传播）、关键组件（激活函数、损失函数、优化器），并简要介绍卷积神经网络（CNN）和循环神经网络（RNN/LSTM/GRU）等重要架构。

## 1. 回顾：什么是深度学习？

如 [AI 基础与历史](/AI/01_基础与历史) 中所述，深度学习是机器学习的一个分支，特指使用具有**多个隐藏层**（即“深层”结构）的**人工神经网络 (Artificial Neural Networks, ANN)**。通过这种深层结构，模型能够从原始数据中自动学习越来越复杂、越来越抽象的特征表示，从而在图像识别、自然语言处理、语音识别等领域取得了革命性的成功。

## 2. 人工神经网络 (ANN) 基础

### 2.1 生物启发与基本单元：神经元 (Neuron)

ANN 的灵感来源于生物大脑中的神经元网络。一个简化的人工神经元（或称为节点、单元）接收一个或多个输入信号，对这些信号进行加权求和，加上一个偏置项，然后通过一个**激活函数 (Activation Function)** 处理，最终产生一个输出信号。

```mermaid
graph LR
    x1 -- w1 --> N(神经元)
    x2 -- w2 --> N
    xn -- wn --> N
    b(偏置 b) --> N
    N -- 激活函数 f --> output(输出 y)

    subgraph Neuron Internals
        direction LR
        Input(加权求和 z = Σ(wi*xi) + b) --> Activation(激活函数 y = f(z))
    end

    N --- Input
    Input --- Activation

    style N fill:#ccf,stroke:#333,stroke-width:2px
```

- **输入 (Inputs, x):** 来自上一层神经元或原始数据。
- **权重 (Weights, w):** 每个输入连接都有一个权重，表示该输入的重要性。**权重是模型需要学习的关键参数。**
- **偏置 (Bias, b):** 一个额外的可学习参数，用于调整神经元的激活阈值。
- **加权和 (Weighted Sum, z):** `z = (w1*x1 + w2*x2 + ... + wn*xn) + b`
- **激活函数 (Activation Function, f):** 对加权和进行非线性变换，`y = f(z)`。非线性是神经网络能够学习复杂模式的关键。如果不用激活函数或只用线性激活函数，那么深层网络本质上就等同于一个单层网络。

### 2.2 网络结构：层 (Layers)

神经元被组织成**层 (Layers)**：

- **输入层 (Input Layer):** 接收原始数据特征。神经元数量通常等于输入数据的维度。
- **隐藏层 (Hidden Layers):** 位于输入层和输出层之间，可以有一层或多层（深度学习通常指有多层隐藏层）。这些层负责提取数据中越来越复杂的特征。
- **输出层 (Output Layer):** 产生最终的预测结果。神经元数量和激活函数取决于具体任务（例如，回归任务通常有一个输出神经元且可能无激活或用线性激活；二分类任务通常有一个输出神经元用 Sigmoid 激活；多分类任务通常有 N 个输出神经元用 Softmax 激活）。

**全连接层 (Fully Connected Layer / Dense Layer):** 一种常见的层类型，其中每个神经元都与前一层的所有神经元相连接。

**深度神经网络 (Deep Neural Network, DNN):** 指包含多个隐藏层的神经网络。

```mermaid
graph TD
    subgraph Input Layer
        I1(Input 1)
        I2(Input 2)
        In(...)
    end
    subgraph Hidden Layer 1
        H1_1 --- H1_2 --- H1_n(...)
    end
    subgraph Hidden Layer 2
        H2_1 --- H2_2 --- H2_m(...)
    end
    subgraph Output Layer
        O1(Output 1)
        O2(...)
    end

    Input Layer --> Hidden Layer 1 --> Hidden Layer 2 --> Output Layer

    style Input Layer fill:#eee,stroke:#333,stroke-width:1px
    style Output Layer fill:#eee,stroke:#333,stroke-width:1px
```

_(这是一个简化的示意图，实际连接是跨层全连接的)_

## 3. 神经网络的工作流程

神经网络的学习过程主要包括两个阶段：**前向传播 (Forward Propagation)** 和 **反向传播 (Backward Propagation)**。

### 3.1 前向传播 (Forward Propagation)

信息从输入层开始，逐层向前传递，直到输出层，计算出模型的预测结果。

1.  将输入数据 `X` 提供给输入层。
2.  对于每个隐藏层和输出层：
    - 计算该层每个神经元的加权和 `z = W * a_prev + b` （`a_prev` 是上一层的激活输出，`W` 和 `b` 是当前层的权重和偏置）。
    - 将加权和通过该层的激活函数 `f` 得到该层的输出 `a = f(z)`。
3.  输出层产生的最终 `a` 即为模型的预测结果 `y_pred`。

### 3.2 损失函数 (Loss Function / Cost Function)

为了衡量模型预测结果 `y_pred` 与真实标签 `y_true` 之间的差距，我们需要定义一个**损失函数 (Loss Function)**，也称为成本函数或目标函数。损失函数的值越小，表示模型预测得越准确。

常用的损失函数：

- **均方误差 (Mean Squared Error, MSE):** 主要用于回归任务。
  <div class="formula">L = (1/n) * Σ(y_true - y_pred)^2</div>
- **交叉熵损失 (Cross-Entropy Loss):** 主要用于分类任务。
  - **二元交叉熵 (Binary Cross-Entropy):** 用于二分类。
    <div class="formula">L = -(1/n) * Σ [ y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred) ]</div>
  - **分类交叉熵 (Categorical Cross-Entropy):** 用于多分类（标签是 one-hot 编码）。
    <div class="formula">L = -(1/n) * Σ Σ [ y_true_ij * log(y_pred_ij) ]</div> (i 遍历样本, j 遍历类别)

### 3.3 反向传播 (Backward Propagation) 与梯度下降 (Gradient Descent)

神经网络学习的目标是找到一组最优的权重 `W` 和偏置 `b`，使得损失函数 `L` 最小化。这个过程通过**反向传播**和**梯度下降**实现。

1.  **计算梯度 (Gradient):** 反向传播算法利用微积分中的**链式法则 (Chain Rule)**，从输出层开始，逐层向后计算损失函数 `L` 相对于网络中每个参数（权重 `W` 和偏置 `b`）的**梯度 (偏导数)**。梯度表示了损失函数在当前参数点上变化最快的方向和速率。`∂L/∂W`, `∂L/∂b`。
2.  **更新参数:** 梯度下降是一种优化算法。它根据计算出的梯度，沿着梯度的**负方向**（即损失下降最快的方向）更新参数，以期逐步减小损失函数的值。
    <div class="formula">W_new = W_old - η * (∂L/∂W_old)</div>
    <div class="formula">b_new = b_old - η * (∂L/∂b_old)</div>
    其中 `η` (eta) 是**学习率 (Learning Rate)**，一个超参数，控制每次更新的步长大小。

**训练过程:** 神经网络的训练就是不断重复**前向传播计算预测和损失** -> **反向传播计算梯度** -> **梯度下降更新参数** 的过程，通常在整个训练数据集上进行多轮（Epochs）迭代，直到损失收敛或达到预设的停止条件。

## 4. 关键组件详解

### 4.1 激活函数 (Activation Functions)

激活函数为神经网络引入非线性，使其能够学习复杂的数据模式。

- **Sigmoid:** `f(z) = 1 / (1 + exp(-z))`
  - 输出范围: (0, 1)。常用于二分类的输出层。
  - 缺点: 输出不是零中心；在输入值很大或很小时梯度接近于零（梯度消失），导致学习缓慢。
- **Tanh (双曲正切):** `f(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))`
  - 输出范围: (-1, 1)。输出是零中心的。
  - 缺点: 仍然存在梯度消失问题。
- **ReLU (Rectified Linear Unit):** `f(z) = max(0, z)`
  - 输出范围: [0, +∞)。计算简单，收敛速度快，是目前最常用的激活函数之一。
  - 缺点: 输出不是零中心；可能导致“神经元死亡”（如果输入恒为负，梯度恒为零，参数不再更新）。
- **Leaky ReLU:** `f(z) = max(αz, z)` (α 是一个小的正常数，如 0.01)
  - ReLU 的变种，解决了神经元死亡问题。
- **Softmax:** 通常用于多分类任务的输出层。将一组任意实数转换为概率分布（所有输出值非负且和为 1）。
  <div class="formula">f(z_i) = exp(z_i) / Σ exp(z_j)</div> (j 遍历该层所有神经元)

### 4.2 优化器 (Optimizers)

优化器是梯度下降算法的具体实现和改进，用于更有效地更新网络参数。

- **随机梯度下降 (Stochastic Gradient Descent, SGD):** 每次只用**一个**训练样本（或一小批样本，称为 Mini-batch SGD）来计算梯度并更新参数。相比于使用整个数据集（Batch GD），SGD 更新更频繁，可能更快收敛，并有助于跳出局部最优，但更新方向可能不稳定。
- **Momentum:** 在 SGD 基础上引入“动量”概念，累积过去的梯度方向，使得更新方向更稳定，加速收敛。
- **AdaGrad (Adaptive Gradient):** 为每个参数维护一个独立的学习率，对稀疏梯度（不常更新的参数）给予较大的学习率，对频繁更新的参数给予较小的学习率。
- **RMSprop:** AdaGrad 的改进，解决了学习率可能过早衰减的问题。
- **Adam (Adaptive Moment Estimation):** 目前最常用的优化器之一。结合了 Momentum 和 RMSprop 的优点，为每个参数计算自适应的学习率，并利用梯度的动量。通常收敛速度快且效果好。

## 5. 重要神经网络架构

除了基本的多层全连接网络（也称多层感知机 MLP），深度学习领域发展出了许多针对特定数据类型和任务的专用架构。

### 5.1 卷积神经网络 (Convolutional Neural Network, CNN)

- **核心思想:** 受到生物视觉皮层的启发，特别擅长处理网格状数据，尤其是**图像**。利用**卷积核 (Kernel/Filter)** 在输入数据上滑动，通过**局部连接 (Local Connectivity)** 和**权值共享 (Weight Sharing)** 来提取空间层级特征。
- **关键层:**
  - **卷积层 (Convolutional Layer):** 使用卷积核提取局部特征，生成特征图 (Feature Map)。权值共享大大减少了模型参数量。
  - **池化层 (Pooling Layer):** 通常在卷积层之后，对特征图进行下采样（如最大池化 Max Pooling 或平均池化 Average Pooling），降低特征图维度，减少计算量，并提高模型的平移不变性。
  - **全连接层 (Fully Connected Layer):** 通常在网络的最后阶段，将前面提取到的高级特征整合起来，用于最终的分类或回归。
- **应用:** 图像分类、目标检测、图像分割、人脸识别等计算机视觉任务。

### 5.2 循环神经网络 (Recurrent Neural Network, RNN)

- **核心思想:** 专门用于处理**序列数据**（如文本、时间序列、语音）。网络中的连接形成一个**循环**，使得信息可以在序列的不同时间步之间传递和保持（具有“记忆”能力）。
- **工作方式:** 在处理序列的每个元素时，RNN 不仅考虑当前的输入，还考虑来自前一个时间步的隐藏状态 (Hidden State)。
  <div class="formula">h_t = f(W_hh * h_{t-1} + W_xh * x_t + b_h)</div>
  <div class="formula">y_t = g(W_hy * h_t + b_y)</div>
  其中 `h_t` 是时间步 t 的隐藏状态，`x_t` 是输入，`y_t` 是输出。权重 `W_hh`, `W_xh`, `W_hy` 在所有时间步共享。
- **挑战:**
  - **梯度消失/爆炸 (Vanishing/Exploding Gradients):** 在处理长序列时，梯度在反向传播过程中可能变得非常小（消失）或非常大（爆炸），导致难以学习长期依赖关系。
- **应用:** 自然语言处理（语言模型、机器翻译、情感分析）、语音识别、时间序列预测。

### 5.3 长短期记忆网络 (LSTM) 与门控循环单元 (GRU)

- **核心思想:** RNN 的改进版本，专门设计用来解决梯度消失问题，从而更好地捕捉序列中的**长期依赖关系**。它们通过引入**门控机制 (Gating Mechanisms)** 来控制信息在网络中的流动和遗忘。
- **LSTM (Long Short-Term Memory):** 包含三个关键的门：
  - **遗忘门 (Forget Gate):** 决定从细胞状态 (Cell State) 中丢弃哪些信息。
  - **输入门 (Input Gate):** 决定哪些新信息存入细胞状态。
  - **输出门 (Output Gate):** 决定基于细胞状态输出什么。
    还有一个**细胞状态 (Cell State)**，作为信息传递的“传送带”。
- **GRU (Gated Recurrent Unit):** LSTM 的简化版本，只有两个门（更新门和重置门），参数更少，计算效率可能更高，在某些任务上效果与 LSTM 相当。
- **应用:** 在许多需要处理长序列的任务上取代了基本的 RNN，例如机器翻译、语音识别、文本生成等。直到 Transformer 架构出现之前，它们是处理序列数据的标准方法。

## 6. 总结

深度学习通过构建深层的人工神经网络，利用前向传播计算预测，通过损失函数衡量误差，再利用反向传播和梯度下降（及其优化算法如 Adam）来学习和调整网络参数（权重和偏置）。激活函数的非线性是其学习复杂模式的关键。针对不同类型的数据，发展出了专门的架构，如用于图像的 CNN 和用于序列数据的 RNN/LSTM/GRU。理解这些基础知识是掌握现代 AI 技术（包括更先进的 Transformer 和 LLM）的基石。

## 7. 参考资料

- [深度学习 (Deep Learning Book by Goodfellow, Bengio, Courville)](https://www.deeplearningbook.org/) - 深度学习领域的权威教材
- [Neural Networks and Deep Learning (Michael Nielsen)](http://neuralnetworksanddeeplearning.com/) - 优秀的在线入门教程
- [CS231n: Convolutional Neural Networks for Visual Recognition (Stanford Course)](http://cs231n.stanford.edu/)
- [CS224n: Natural Language Processing with Deep Learning (Stanford Course)](http://web.stanford.edu/class/cs224n/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) / [TensorFlow Documentation](https://www.tensorflow.org/api_docs) - 流行的深度学习框架文档
