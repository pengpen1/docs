<h1 align="center" id="微调模型">微调模型</h1>

**概要：** 本章节将介绍如何微调大语言模型，包括微调的基本概念、常见微调方法以及具体实现技术。

## 微调概述

微调（Fine-tuning）是一种将预训练模型适应特定下游任务的技术。对于大语言模型来说，微调使模型能够专注于特定领域或任务，提高在该领域的表现。微调的优势在于能够利用预训练模型已学习的知识，在较小的特定领域数据集上快速调整，使模型适应新的任务需求。

### 微调的类型

1. **全量微调（Full Fine-tuning）**：调整模型的所有参数，计算资源需求高，通常需要较大的GPU内存。
2. **参数高效微调（PEFT, Parameter-Efficient Fine-Tuning）**：只调整模型的部分参数，大大减少计算资源需求，同时保持模型性能。

### 参数高效微调方法分类

参数高效微调方法可以分为三大类：

1. **加性方法（Additive Methods）**：向基础模型添加新参数。
   - Adapters：在特定transformer子层后插入小型全连接网络。
   - Soft Prompts：直接对模型的输入嵌入进行微调，如Prompt Tuning、Prefix Tuning、P-tuning等。
   - (IA)³：通过抑制和放大内部激活来调整模型。

2. **重参数化方法（Reparameterization-based Methods）**：利用低秩表示减少可训练参数。
   - Intrinsic SAID：利用Fastfood变换来表示低秩更新。
   - LoRA：采用低秩矩阵分解来表示权重更新。
   - KronA：利用Kronecker积实现参数高效调整。

3. **选择性方法（Selective Methods）**：仅调整模型现有参数的一个子集。
   - BitFit：只微调偏置项。
   - DiffPruning：通过可学习的二进制掩码实现稀疏更新。
   - FishMask：基于Fisher信息选择要调整的参数。

## LoRA微调

LoRA（Low-Rank Adaptation）是一种参数高效的微调技术，由Hu等人在2021年提出。它通过低秩矩阵分解来降低预训练模型微调时需要更新的参数数量，同时保持模型性能。

### LoRA的工作原理

LoRA的核心思想是冻结预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层，从而大大减少下游任务的可训练参数数量。

1. **权重更新分解**：在传统微调中，我们直接更新权重矩阵W。而在LoRA中，更新通过两个低秩矩阵A和B的乘积来表示：ΔW = A × B

2. **前向传播**：对于输入x，输出计算如下：
   y = Wx + ΔWx = Wx + (A × B)x

3. **参数高效性**：假设原始权重矩阵W的维度为(d, d)，而A的维度为(d, r)，B的维度为(r, d)，其中r远小于d。通过这种方式，可训练参数的数量从d²减少到2 × r × d。

### LoRA的优势

- **内存效率**：由于只需要更新少量参数，大大减少了内存使用。
- **计算效率**：训练速度更快，所需计算资源更少。
- **模块化**：可以为不同任务训练不同的LoRA适配器，共享同一个基础模型。
- **无推理延迟**：适配器权重可以在推理前合并到基础模型中，不会增加推理时间。

### LoRA的改进变体

1. **QLoRA（Quantized Low-Rank Adaptation）**：
   - 结合量化技术和LoRA，进一步降低内存使用。
   - 使用4位Normal Float (NF4)数据类型存储模型权重。
   - 实现了双量化(Double Quantization)，通过量化量化常数来减少内存使用。
   - 可以在单个48GB GPU上微调高达650亿参数的模型。

2. **DoRA（Weight-Decomposed Low-Rank Adaptation）**：
   - 将权重更新分解为量级和方向两个组件。
   - 方向由标准LoRA处理，量级由单独的可学习参数管理。
   - 在低秩设置下性能优于标准LoRA。

3. **LoHa（Low-Rank Hadamard Product）**：
   - 使用Hadamard积（元素级乘法）代替传统的矩阵乘法。
   - 将权重更新矩阵ΔW分解为四个较小的矩阵。

4. **LoKr（Low-Rank Kronecker Product）**：
   - 使用Kronecker积代替传统的矩阵乘法。
   - A⊗B产生一个块矩阵，保留原始权重矩阵的结构和秩。

5. **X-LoRA（Mixture of LoRA Experts）**：
   - 利用"专家混合"（MoE）概念，动态激活不同的LoRA专家。
   - 使用控制层（门）决定激活哪些LoRA专家，可以是密集或稀疏的。
   - 训练期间只训练控制层，而LoRA专家和基础模型都保持冻结状态。

## 使用PEFT和LoRA进行模型微调的实现

下面是使用PEFT库实现LoRA微调的基本步骤：

### 1. 安装所需库

```python
pip install peft transformers datasets accelerate torch bitsandbytes
```

### 2. 加载预训练模型和数据集

```python
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

# 加载数据集
ds = Dataset.load_from_disk("./your_dataset")

# 加载分词器和模型
tokenizer = AutoTokenizer.from_pretrained("your_model_name")
model = AutoModelForCausalLM.from_pretrained("your_model_name", low_cpu_mem_usage=True)
```

### 3. 数据处理

```python
def process_func(example):
    MAX_LENGTH = 256
    input_ids, attention_mask, labels = [], [], []
    
    # 处理输入和输出
    instruction = tokenizer("\n".join(["Human: " + example["instruction"], example["input"]]).strip() + "\n\nAssistant: ")
    response = tokenizer(example["output"] + tokenizer.eos_token)
    
    # 组合token ids
    input_ids = instruction["input_ids"] + response["input_ids"]
    attention_mask = instruction["attention_mask"] + response["attention_mask"]
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"]
    
    # 截断过长序列
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# 处理数据集
tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)
```

### 4. 配置LoRA微调

```python
from peft import LoraConfig, TaskType, get_peft_model

# 配置LoRA参数
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"], # 目标模块
    inference_mode=False,  # 训练模式
    r=8,  # LoRA秩
    lora_alpha=32,  # 缩放因子
    lora_dropout=0.1  # LoRA层的dropout率
)

# 将LoRA应用于模型
model = get_peft_model(model, config)

# 打印可训练参数
model.print_trainable_parameters()
```

### 5. 配置训练参数并开始训练

```python
from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer

# 配置训练参数
args = TrainingArguments(
    output_dir="./output_model",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    learning_rate=5e-4,
    logging_steps=10,
    num_train_epochs=3
)

# 创建训练器
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_ds,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

# 开始训练
trainer.train()
```

### 6. 模型推理

```python
# 加载模型到GPU
model = model.cuda()

# 准备输入
user_input = "你好，请介绍一下自然语言处理。"
input_text = f"Human: {user_input}\n\nAssistant: "
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

# 生成回复
outputs = model.generate(**inputs, max_length=512, do_sample=True, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)
```

### 7. 合并LoRA权重（可选）

```python
# 合并LoRA权重到基础模型
merged_model = model.merge_and_unload()

# 保存合并后的模型
merged_model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
```

## 参考

- [LangChain](https://panda-99.com/zh-cn/posts/langchain/)
- [PEFT: Parameter-Efficient Fine-Tuning Methods for LLMs](https://huggingface.co/blog/samuellimabraz/peft-methods)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)